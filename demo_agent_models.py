


"""
Demonstration script showing different agents using different LLM models.
This uses a mock mode that doesn't require actual API calls.
"""

from config.agent_config import get_agent_registry, set_default_model_for_agent
from agents.reflection.document_summarizer import DocumentSummarizer
from agents.llm_client import get_available_models

# Mock the LLM responses for demonstration purposes
def mock_llm_response(model_name: str, prompt: str) -> str:
    """Generate mock responses based on model name"""
    if "gpt-4" in model_name:
        return f"[GPT-4 Response] This is a concise summary generated by GPT-4. The text discusses AI and intelligent agents."
    elif "claude-2" in model_name:
        return f"[Claude-2 Response] This is a detailed summary from Claude-2. Artificial intelligence involves machines demonstrating intelligence."
    elif "llama-2-7b" in model_name:
        return f"[Llama-2 Response] Short summary from Llama-2: AI is machine intelligence vs natural intelligence."
    else:
        return f"[Generic Response] Summary generated by {model_name}. The text is about artificial intelligence."

def test_agent_models():
    """Test different agents using different models"""
    print("Available LLM models:", get_available_models())
    print("Registered agents:", get_agent_registry().list_agents())
    print()

    # Create a DocumentSummarizer with default model
    summarizer1 = DocumentSummarizer()
    print(f"Summarizer1 model: {summarizer1.get_model_name()}")
    print()

    # Create a DocumentSummarizer with specific model
    summarizer2 = DocumentSummarizer(model_name="claude-2")
    print(f"Summarizer2 model: {summarizer2.get_model_name()}")
    print()

    # Test summarization with different models
    test_text = """
    Artificial intelligence (AI) is intelligence demonstrated by machines,
    in contrast to the natural intelligence displayed by humans and animals.
    Leading AI textbooks define the field as the study of "intelligent agents":
    any device that perceives its environment and takes actions that maximize
    its chance of successfully achieving its goals.
    """

    print("Testing summarization with different models...")
    print()

    # Test with summarizer1 (uses default model)
    result1 = summarizer1.generate_summary(test_text)
    print(f"Summarizer1 result (model: {result1['model_used']}):")
    print(f"Summary: {mock_llm_response(result1['model_used'], test_text)}")
    print()

    # Test with summarizer2 (uses claude-2 model)
    result2 = summarizer2.generate_summary(test_text)
    print(f"Summarizer2 result (model: {result2['model_used']}):")
    print(f"Summary: {mock_llm_response(result2['model_used'], test_text)}")
    print()

    # Test overriding model at runtime
    result3 = summarizer1.generate_summary(test_text, model_name="llama-2-7b")
    print(f"Summarizer1 with override (model: {result3['model_used']}):")
    print(f"Summary: {mock_llm_response(result3['model_used'], test_text)}")
    print()

    # Change default model for agent and test again
    set_default_model_for_agent("DocumentSummarizer", "llama-2-7b")
    summarizer3 = DocumentSummarizer()
    print(f"Summarizer3 model (after config change): {summarizer3.get_model_name()}")
    result4 = summarizer3.generate_summary(test_text)
    print(f"Summarizer3 result (model: {result4['model_used']}):")
    print(f"Summary: {mock_llm_response(result4['model_used'], test_text)}")

    print("\nDemonstration complete!")
    print("This shows how different agents can use different LLM models.")
    print("In a real implementation, the actual LLM API calls would be made.")

if __name__ == "__main__":
    test_agent_models()


